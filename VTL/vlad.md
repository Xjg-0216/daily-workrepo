### VLAD
Vector of local aggregated descriptors, 是图像特征提取方法的一种

图像检索经典思路：
* 存在一个图像库$I$, 对每张图像$I_i$通过特征函数提取特征$f(I_i)$
* 提供一张query图像$q$，通过特征函数提取特征$f(q)$
* 将query特征$f(q)$与图库特征$f(I)$做相似度，一般为欧式距离：$d(q, I)=||f(q)-f(I)||$

VLAD是特征提取函数的一种，如其描述--局部聚合向量，将局部特征聚类得到一个向量。所以VLAD应用的前提是先获取局部特征。

>局部特征可以用SIFT， SURF， ORB等一般方法，也可以使用CNN。

假设一张图像，提取了$N$个$D$维局部特征
VLAD步骤如下：
* 1. 对全部的$N \times D$维特征图进行聚类（K-Means），获得K个聚类中心，记为$C_k$
* 2. 将$N \times D$维局部特征图转为一个全局特征图$V$, 全局特征图大小为$K\times D$，公式如下：
$V(j, k) = \sum^N_{i=1}a_k(x_i)(x_i(j)-c_k(j)), k\in K, j\in D$

公式中$x_i$表示第$i$个局部特征，$c_k$表示第k个聚类中心，$x_i$和$c_k$都是D维向量。$a_k(x_i)$是一个符号函数，如果$x_i$不属于聚类中心$c_k$， $a_k(x_i) = 0$;如果$x_i$属于聚类中心$c_k$， $a_k(x_i)=1$;

可以看出该式累加了每个聚类的所有特征残差，最终得到了K个全局特征，这K个全局特征表达了聚类范围内局部特征的某种分布，这种分布通过$x_i-c_k$抹去了图像本身的特征分布差异，只保留了局部特征与聚类中心的分布差异。

### NetVLAD

VLAD是一个不可导的函数（$a_k(x_i)$符号函数），为了让VLAD变得可导，需要将其变得平滑，可微，根据$a_k(x_i)$的特性，将其平滑为一个权重函数，即$x_i$与$c_k$越相近，$a_k(x_i)$越接近1， 反之越接近0。可设计为公式
$\hat{a}(x_i)=\frac{e^{-\alpha||x_i-c_k||^2}}{\sum_{k^`}e^{-\alpha||x_i-c_{k^`}||^2}} \in (0, 1)$

>其实该函数为softmax函数，下面给出Softmax函数的定义（以第i个节点输出为例）：
$softmax(z_i) = \frac{e^{z_i}}{\sum^C_{c=1}e^{z_c}}$，其中 $z_i$为第$i$个节点的输出值，$C$为输出节点的个数，即分类的类别个数。通过Softmax函数就可以将多分类的输出值转换为范围在0-1的概率分布。
Softmax的含义就在于不再唯一的确定某一个最大值，而是为每个输出分类的结果都赋予一个概率值，表示属于每个类别的可能性。

对公式进行化简， 将2范数进行展开，分子分母同时约去$e^{-\alpha ||x||^2}$后可得，
$\hat{a}(x_i)=\frac{e^{w^T_kx_i+b_k}}{\sum_{k^`}e^{w^T_{k^`} x_i + b_{k^`}}}(x_i(j)-c_k(j))$

这里$w_k, b_k, c_k$是NetVLAD要学习的参数。从VLAD到NetVLAD的最大变化是之前需要通过聚类获得参数$c_k$变成了需要通过训练得到。这样可以把VLAD变成了一个分类问题，即设定有K个分类，计算局部特征在这K个分类的差值分布来得到全局特征$V(j, k)$。
![alt text](<Screenshot from 2024-05-22 15-05-46.png>)






